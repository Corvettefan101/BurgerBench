# üçî BurgerBench: AI Code Generation Benchmarking Tool

BurgerBench is a lightweight, client-side web application designed to benchmark the code generation capabilities of various Large Language Models (LLMs) from different API providers. Specifically, it focuses on evaluating how quickly and efficiently these models can generate a complete HTML file based on a predefined prompt.

This tool is perfect for developers and AI enthusiasts who want to compare the performance of different LLMs in a practical code generation scenario directly in their browser.

## ‚ú® Features

*   **Multi-Provider Support:** Seamlessly integrate and test models from:
    *   **OpenAI** (e.g., GPT-3.5, GPT-4 series)
    *   **OpenRouter** (access to a wide range of open-source and commercial models)
    *   **Anthropic** (e.g., Claude series)
    *   **Google Gemini** (e.g., Gemini 1.5 Flash/Pro)
*   **Key Performance Metrics:** Measure and visualize crucial LLM performance benchmarks:
    *   **Generation Speed (s):** The total time taken for the API to return the complete HTML output.
    *   **Tokens Used:** The total number of tokens generated by the model for the given prompt. This is a key indicator for API costs.
    *   **Tokens/Second:** The throughput of the model, indicating how many tokens are generated per second.
*   **Standardized Prompt:** All benchmarks run against a fixed, consistent prompt (`"Make me a website for my burger restaurant that looks modern with good colors in a single html file"`) ensuring fair comparison across models.
*   **Interactive Results:**
    *   **Visual Chart:** A bar chart provides an immediate, comparative overview of selected models' performance.
    *   **Detailed Cards:** Each tested model gets a dedicated card displaying its individual metrics.
    *   **HTML Preview:** See the generated HTML rendered live within an iframe.
    *   **Raw HTML View:** Access and review the raw HTML code generated by the LLM.
*   **Convenient Utilities:**
    *   **Copy to Clipboard:** Easily copy the generated HTML to your clipboard.
    *   **Download HTML:** Download the generated HTML as a `.html` file for local inspection or use.
*   **Local Persistence:** API keys and selected models are saved securely in your browser's local storage and are not sent to any external DB, so you don't have to re-enter them on every visit. This is true for both the local and hosted version of BurgerBench

## üöÄ Getting Started

To use BurgerBench, simply open the `index.html` file in your web browser or visit our hosted version https://burgerbench.pages.dev/

### Prerequisites

*   A modern web browser (Chrome, Firefox, Edge, Safari).
*   API keys for the LLM providers you wish to test (OpenAI, OpenRouter, Anthropic, Google Gemini).

### Installation

1.  Clone this repository:
    ```bash
    git clone https://github.com/Corvettefan101/BurgerBench.git
    cd BurgerBench
    ```
2.  Open the `index.html` file in your browser:
    ```bash
    open index.html # On macOS
    # or start index.html # On Windows
    # or google-chrome index.html # On Linux (or similar for your browser)
    ```
    Or just download it in the releases tab

### Usage

1.  **Navigate to Settings:** Click the "Settings" button in the header.
2.  **Enter API Keys:** Input your API keys for the desired providers. Click "Fetch" / "Check" / "Load" next to each key to retrieve the available models for that provider.
3.  **Select Models:** Choose one or more models from the fetched lists by checking their respective checkboxes.
4.  **Save Settings:** Click the "Save Settings" button to persist your choices.
5.  **Run Benchmark:** Return to the "Main" page and click the "Run Test" button. BurgerBench will send the predefined prompt to all selected models and display the results.
6.  **Review Results:** Analyze the charts and individual model cards to compare performance. Use the "Preview" and "HTML" tabs to inspect the generated code, and the copy/download buttons for convenience.

## üìà Benchmarks Explained

BurgerBench provides the following key metrics for each tested LLM:

*   **Generation Speed (s):**
    *   **What it is:** The total time (in seconds) taken from sending the API request to receiving the complete response from the LLM.
    *   **Why it matters:** Directly impacts the end-user waiting time.
*   **Tokens Used:**
    *   **What it is:** The number of output tokens (words or sub-words) generated by the LLM in response to the prompt.
    *   **Why it matters:** Directly correlates with the cost of using the API (most LLM APIs charge per token). Lower token count for equivalent quality means lower operational cost.
*   **Tokens/Second:**
    *   **What it is:** The rate at which the LLM generates tokens (tokens used divided by generation speed).
    *   **Why it matters:** Indicates the raw processing throughput of the model. Higher tokens/second means faster code streaming or quicker batch processing.

## ü§ù Contributing

Contributions are welcome! If you have suggestions for improvements, bug fixes, or new features, please feel free to DM Corvettefan101 #1738 on Discord or open a PR! 

## ‚ú® Credits

*   **Code:** Gemini 2.5 Flash and Gemini 2.5 Pro for generating the code based on requirements.
*   **Testing & Implementation:** Corvettefan101

## üìÑ License

This project is open-source and available under the [MIT License](LICENSE).

---

**Happy Benchmarking!** üçî
